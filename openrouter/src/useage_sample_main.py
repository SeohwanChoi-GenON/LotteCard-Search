import asyncio

from client.openrouter.multi_model_manager import MultiModelManager
from settings import get_settings

class OpenRouterConfig:
    """OpenRouter ì„¤ì •ì„ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë¡œ ê´€ë¦¬"""

    def __init__(self):
        # **ëª¨ë¸ ì„ íƒ** (ì‚¬ìš©í•˜ê³  ì‹¶ì€ ëª¨ë¸ë“¤)
        self.selected_models = [
            "claude-4-sonnet",
            "gpt-4.1-mini",
            "qwen3-next-80b",
            "gpt-oss"
        ]

        # **ë©”ì‹œì§€ ì„¤ì •**
        self.user_message = "Supervisor Agentì— ëŒ€í•´ì„œ í•µì‹¬ ë‚´ìš©ë§Œ ì•Œë ¤ì£¼ì„¸ìš”."

        # **ìš”ì²­ ì„¤ì •**
        self.temperature = 0.2
        self.max_tokens = 200
        self.stream_mode = False  # True: ìŠ¤íŠ¸ë¦¼, False: ì¼ë°˜

        # **í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„ íƒ**
        self.test_mode = "single"  # "single", "compare", "stream", "provider"


async def run_single_model(config: OpenRouterConfig):
    """ë‹¨ì¼ ëª¨ë¸ ì‹¤í–‰"""
    print("=== **ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸** ===")

    settings = get_settings()
    manager = MultiModelManager(settings)

    # ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©
    selected_model = config.selected_models[0]
    messages = [{"role": "user", "content": config.user_message}]

    print(f"**ì‚¬ìš© ëª¨ë¸**: {selected_model}")
    print(f"**ì§ˆë¬¸**: {config.user_message}")
    print("-" * 50)

    if config.stream_mode:
        print("**ìŠ¤íŠ¸ë¦¼ ì‘ë‹µ**:")
        stream_response = await manager.chat_with_model(
            model_name=selected_model,
            messages=messages,
            temperature=config.temperature,
            max_tokens=config.max_tokens,
            stream=True
        )

        content = ""
        async for chunk in stream_response:
            if "choices" in chunk and chunk["choices"]:
                delta = chunk["choices"][0].get("delta", {})
                if "content" in delta:
                    chunk_content = delta["content"]
                    print(chunk_content, end="", flush=True)
                    content += chunk_content

                finish_reason = chunk["choices"][0].get("finish_reason")
                if finish_reason:
                    print(f"\n\n**ì™„ë£Œ**: {finish_reason}")
                    break
    else:
        response = await manager.chat_with_model(
            model_name=selected_model,
            messages=messages,
            temperature=config.temperature,
            max_tokens=config.max_tokens,
            stream=False
        )

        content = response["choices"][0]["message"]["content"]
        usage = response.get("usage", {})

        print(f"**ì‘ë‹µ**: {content}")
        print(f"**í† í° ì‚¬ìš©ëŸ‰**: {usage}")


async def run_model_comparison(config: OpenRouterConfig):
    """ëª¨ë¸ ë¹„êµ ì‹¤í–‰"""
    print("=== **ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸** ===")

    settings = get_settings()
    manager = MultiModelManager(settings)

    messages = [{"role": "user", "content": config.user_message}]

    print(f"**ë¹„êµ ëª¨ë¸ë“¤**: {config.selected_models}")
    print(f"**ì§ˆë¬¸**: {config.user_message}")
    print("-" * 50)

    results = await manager.compare_models(
        model_names=config.selected_models,
        messages=messages,
        temperature=config.temperature,
        max_tokens=config.max_tokens
    )

    for i, (model_name, result) in enumerate(results.items(), 1):
        print(f"\n**{i}. {model_name}**")
        if "error" in result:
            print(f"âŒ ì˜¤ë¥˜: {result['error']}")
        else:
            print(f"âœ… ì‘ë‹µ: {result['response']}")
            print(f"ğŸ“Š í† í°: {result.get('usage', {})}")
        print("-" * 30)


async def run_stream_multiple(config: OpenRouterConfig):
    """ë‹¤ì¤‘ ëª¨ë¸ ìŠ¤íŠ¸ë¦¼ ì‹¤í–‰"""
    print("=== **ë‹¤ì¤‘ ëª¨ë¸ ìŠ¤íŠ¸ë¦¼ í…ŒìŠ¤íŠ¸** ===")

    settings = get_settings()
    manager = MultiModelManager(settings)

    messages = [{"role": "user", "content": config.user_message}]

    print(f"**ìŠ¤íŠ¸ë¦¼ ëª¨ë¸ë“¤**: {config.selected_models}")
    print(f"**ì§ˆë¬¸**: {config.user_message}")
    print("-" * 50)

    await manager.stream_multiple_models(
        model_names=config.selected_models,
        messages=messages,
        temperature=config.temperature,
        max_tokens=config.max_tokens
    )


async def run_provider_analysis(config: OpenRouterConfig):
    """ì œê³µìë³„ ë¶„ì„ ì‹¤í–‰"""
    print("=== **ì œê³µìë³„ ëª¨ë¸ ë¶„ì„** ===")

    settings = get_settings()
    manager = MultiModelManager(settings)

    providers = ["openai", "anthropic", "qwen", "google"]

    for provider in providers:
        models = manager.models.get_model_by_provider(provider)
        print(f"**{provider.upper()} ëª¨ë¸ë“¤**:")
        for model in models:
            print(f"  - {model}")
        print()


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì„¤ì •ì„ í•˜ë‚˜ì˜ ë³€ìˆ˜ë¡œ ê´€ë¦¬"""

    # ========================================
    # ğŸ¯ **ì—¬ê¸°ì„œ ëª¨ë“  ì„¤ì •ì„ ê´€ë¦¬í•˜ì„¸ìš”!**
    # ========================================

    config = OpenRouterConfig()

    # **ì›í•˜ëŠ” ì„¤ì •ìœ¼ë¡œ ë³€ê²½**
    config.selected_models = [
        "claude-4-sonnet",
        "gpt-4.1-mini",
        "qwen3-next-80b",
        "gpt-oss"
    ]

    config.user_message = "Supervisor Agentì— ëŒ€í•´ì„œ í•µì‹¬ ë‚´ìš©ë§Œ ì•Œë ¤ì£¼ì„¸ìš”."

    config.temperature = 0.2  # ì°½ì˜ì„± (0.0 ~ 1.0)
    config.max_tokens = 200  # ìµœëŒ€ í† í° ìˆ˜
    config.stream_mode = False  # True: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼, False: ì¼ë°˜ ì‘ë‹µ
    config.test_mode = "single"  # "single", "compare", "stream", "provider"

    # ========================================
    # ğŸš€ **ì„ íƒëœ ëª¨ë“œ ì‹¤í–‰**
    # ========================================

    print(f"ğŸ”§ **ì„¤ì • ì •ë³´**")
    print(f"ëª¨ë¸: {config.selected_models}")
    print(f"ì§ˆë¬¸: {config.user_message}")
    print(f"ì˜¨ë„: {config.temperature}, í† í°: {config.max_tokens}")
    print(f"ìŠ¤íŠ¸ë¦¼: {config.stream_mode}, ëª¨ë“œ: {config.test_mode}")
    print("=" * 60)

    # ëª¨ë“œë³„ ì‹¤í–‰
    if config.test_mode == "single":
        await run_single_model(config)

    elif config.test_mode == "compare":
        await run_model_comparison(config)

    elif config.test_mode == "stream":
        config.stream_mode = True  # ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ ê°•ì œ í™œì„±í™”
        await run_stream_multiple(config)

    elif config.test_mode == "provider":
        await run_provider_analysis(config)

    else:
        print("âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í…ŒìŠ¤íŠ¸ ëª¨ë“œì…ë‹ˆë‹¤.")
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“œ: single, compare, stream, provider")


if __name__ == "__main__":
    asyncio.run(main())